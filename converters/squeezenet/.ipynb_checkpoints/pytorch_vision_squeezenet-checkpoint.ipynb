{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is optionally accelerated with a GPU runtime.\n",
    "### If you would like to use this acceleration, please select the menu option \"Runtime\" -> \"Change runtime type\", select \"Hardware Accelerator\" -> \"GPU\" and click \"SAVE\"\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "# SqueezeNet\n",
    "\n",
    "*Author: Pytorch Team*\n",
    "\n",
    "**Alexnet-level accuracy with 50x fewer parameters.**\n",
    "\n",
    "<img src=\"https://pytorch.org/assets/images/squeezenet.png\" alt=\"alt\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SqueezeNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (3): Fire(\n",
       "      (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Fire(\n",
       "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Fire(\n",
       "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (7): Fire(\n",
       "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): Fire(\n",
       "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): Fire(\n",
       "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Fire(\n",
       "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (12): Fire(\n",
       "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'squeezenet1_0', pretrained=True)\n",
    "# or\n",
    "# model = torch.hub.load('pytorch/vision:v0.6.0', 'squeezenet1_1', pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All pre-trained models expect input images normalized in the same way,\n",
    "i.e. mini-batches of 3-channel RGB images of shape `(3 x H x W)`, where `H` and `W` are expected to be at least `224`.\n",
    "The images have to be loaded in to a range of `[0, 1]` and then normalized using `mean = [0.485, 0.456, 0.406]`\n",
    "and `std = [0.229, 0.224, 0.225]`.\n",
    "\n",
    "Here's a sample execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download an example image from the pytorch website\n",
    "import urllib\n",
    "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
    "try: urllib.URLopener().retrieve(url, filename)\n",
    "except: urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5.5942,  5.6852, 10.6783, 10.1641, 10.0021,  6.5080,  7.4874, 18.0157,\n",
      "        22.7080, 11.3663,  7.2890,  8.2363,  6.6445,  9.3229,  4.2166,  4.8906,\n",
      "        12.5777, 13.4572,  9.1000,  9.5117, 10.0890, 15.9781, 14.4654, 17.8954,\n",
      "        10.6867,  3.1573,  2.7917,  4.5262,  3.0540, 15.0804,  3.8555,  2.3249,\n",
      "         3.1948,  6.2436,  5.4086,  3.6793,  7.1638,  5.0132,  3.0156,  5.0269,\n",
      "         2.5425,  2.0304,  2.3658,  4.8843,  3.2687,  3.9885,  3.7628,  3.6088,\n",
      "         6.0392,  4.8689,  4.9457,  7.9475,  3.5956,  4.3305,  5.9926,  3.4121,\n",
      "         4.5425,  2.1517,  2.6229,  3.0778,  2.9012,  6.0712,  6.5218,  3.3208,\n",
      "         3.1252,  3.3029,  5.2317,  5.2004,  3.7301,  1.1604,  1.5400,  0.6113,\n",
      "         2.2134,  2.5150,  2.6014,  2.8081,  5.1842,  2.8636,  5.0886,  2.1452,\n",
      "         6.4154, 12.8008, 10.3866, 10.9838, 15.8780,  9.8128, 12.1769,  7.9432,\n",
      "         7.9127, 15.0490,  4.6106,  6.9495,  4.7531, 11.5708, 10.1503,  5.7562,\n",
      "        10.6124, 11.2838,  5.8174, 14.5051,  8.6572,  5.0427,  7.5822,  6.0318,\n",
      "        27.2462, 16.0573, 14.3964,  3.7681,  9.3995,  7.1425,  7.5566,  3.1255,\n",
      "         8.0220,  3.5603,  7.7556,  6.6786,  2.3657,  4.6438,  2.3773,  2.4094,\n",
      "         1.8758,  3.9914,  1.3137,  2.2332,  2.8840,  4.5735,  1.9751, 11.9522,\n",
      "         7.7149, 14.0440, 10.1906,  8.2630, 12.5049,  4.2513, 11.6588,  6.6980,\n",
      "         4.0306,  6.2917,  8.7617,  6.7627, 10.2365,  5.4898,  3.0158,  7.3417,\n",
      "        14.3740, 14.3250, 17.6083,  2.9685,  6.4677,  9.2071,  5.8995, 27.0677,\n",
      "        28.0279, 25.0048, 25.6117, 22.7784, 22.6275, 31.4396, 24.1833, 11.5749,\n",
      "        20.9676, 16.1299, 18.8483, 10.9779, 13.3518,  7.4153, 16.8536, 16.5032,\n",
      "        11.4979, 25.3510, 21.1894, 17.4241, 18.2005, 25.5344, 25.1968, 17.1450,\n",
      "        18.7955, 15.9619, 13.0110, 22.6500, 21.2688, 15.7815, 17.3749, 14.8574,\n",
      "        17.9428, 23.4578, 23.9511, 17.5980, 24.6295, 18.0547, 24.4331, 13.2158,\n",
      "        25.1853, 23.5424, 20.6028, 21.3253, 20.7881, 17.9774, 17.6505, 25.3398,\n",
      "        23.0990, 18.5083, 20.5490, 31.2539, 23.4940, 15.8702, 10.3031, 23.5105,\n",
      "        21.5825, 12.8473, 13.5572, 11.3474, 23.9761, 14.7260, 13.8155, 23.6928,\n",
      "        23.7697, 23.8910, 22.3383, 22.8421, 20.0580, 10.9765, 28.7425, 22.3752,\n",
      "        20.1077, 18.5084, 19.9638, 23.5154, 19.6344, 24.6984, 29.1814, 30.7751,\n",
      "        30.5375, 15.9068, 11.7574, 22.9895, 14.3891, 12.3556, 20.3447, 24.0428,\n",
      "        22.4817, 19.2384, 19.0315, 15.0215, 19.3198, 21.8829, 16.9541, 24.0791,\n",
      "        31.1876, 30.7426, 30.5570, 22.2899, 15.7328, 21.0083, 18.9231, 16.9751,\n",
      "        20.7320, 31.0856, 35.7870, 30.4056, 24.8472, 28.1278, 15.6842, 28.3688,\n",
      "        29.3262, 21.1547, 21.5579, 22.1784, 14.4712, 24.5735, 30.9853, 18.9530,\n",
      "        17.2080, 23.2346, 18.1932, 17.0870, 14.9138, 20.4209, 18.6501, 31.7604,\n",
      "        19.3230, 22.4632, 21.9743, 26.4565, 28.4012, 23.3457, 15.8670, 24.6011,\n",
      "        10.2386, 17.2309,  9.1427, 13.6483, 18.5454, 13.6538, 12.3199, 12.5859,\n",
      "        19.5127, 13.0721, 12.2946, 13.0074,  0.9721,  4.6042,  1.8865,  1.2621,\n",
      "         2.3782,  2.4907,  3.4804,  1.2623,  3.8126,  8.3645,  5.6673,  2.2516,\n",
      "         1.7008,  3.5067,  1.4940,  2.6386,  2.6825,  1.5155,  2.7769,  1.5018,\n",
      "         1.4386,  5.3575,  6.1695,  3.1427, 10.0770,  3.8084,  3.6799, 10.7823,\n",
      "         7.9648,  8.2471, 19.6563, 21.9281, 28.4220, 23.2250, 12.5687, 16.8736,\n",
      "        14.9820, 11.0515, 24.4922, 11.8314, 11.2075, 20.2896, 12.7634,  9.0975,\n",
      "         6.9463, 16.5188, 10.6244,  6.2467, 20.7290, 14.0782, 11.2206,  6.8028,\n",
      "         7.8790, 13.8132, 11.8097, 23.5937, 24.3676, 15.7987, 22.5793, 23.3959,\n",
      "         9.8149, 16.9525, 16.7358,  8.6898, 12.9319,  9.1593,  9.3384,  7.8204,\n",
      "        18.8894, 10.1870, 17.4988, 17.2675, 14.4585, 17.9246, 18.8255, 15.9450,\n",
      "        13.1526, 19.0049, 17.4477, 10.4538, 20.2789, 14.0953, 15.0462, 19.7722,\n",
      "        18.9686,  5.4668,  4.4125, 15.6644, 20.9777,  8.4974,  2.9944,  6.4273,\n",
      "         1.8784,  5.4945,  6.7954,  5.2466,  3.3931,  8.7793,  5.3143,  6.1782,\n",
      "         6.4042,  5.6838,  6.1840,  3.1394,  6.2735,  5.5112,  3.9531,  5.8845,\n",
      "         6.5732,  4.7223,  9.0532,  6.1872, 12.5767,  6.5456,  4.0105,  7.1514,\n",
      "         6.3414,  6.2225,  5.7934, 10.3888,  8.6452,  5.5068,  6.7020,  4.1288,\n",
      "         7.8138,  6.2680,  3.3430,  5.8066, 16.2778,  8.9304,  3.0678, 13.0884,\n",
      "         8.2671,  4.7750, 15.3060, 13.3675,  6.1980,  7.1042,  2.9024, 10.3959,\n",
      "         5.4090,  5.4555,  7.7894, 11.6195,  6.6086,  6.2950,  4.1049,  9.0729,\n",
      "        10.0436,  4.6186,  9.7468,  2.5209, 14.1914,  9.9564, 11.2888,  7.3597,\n",
      "         6.3721, 16.2986,  5.6400,  6.4577, 10.8516,  7.8521,  8.5030, 17.6379,\n",
      "         3.0100,  5.7616,  4.5170,  5.2439,  5.9868,  8.4699,  4.5326,  9.9182,\n",
      "         6.9660,  4.0120,  9.0724, 10.3888,  9.5509,  2.2315, 17.3481,  7.5994,\n",
      "         7.9419,  3.9272,  4.6237,  8.8800,  3.5171,  5.4727,  5.7396,  9.2642,\n",
      "         3.9599, 10.0451,  8.6603,  7.7978,  5.5853,  1.1466,  5.1050,  5.1030,\n",
      "        14.5922,  8.3005,  5.0078,  5.7861,  4.3312, 13.5027, 10.1978,  5.5583,\n",
      "         6.5042,  5.5715,  4.0315,  2.2109, 15.1407,  3.5803,  1.2003,  6.3716,\n",
      "         1.4867,  7.0343,  5.7378, 12.3412, 11.1365,  2.6263,  5.6274, 11.7818,\n",
      "        10.7058,  6.4522, 10.0606, 10.3378,  7.3753,  8.4837,  5.9848,  6.9500,\n",
      "         3.7586, 11.5862,  5.1510,  5.3345,  6.2423,  4.4713, 16.6494,  3.5048,\n",
      "         1.3161, 17.7486,  4.5629, 20.3672,  0.9254,  5.6450,  4.8691,  8.2169,\n",
      "         5.9502,  3.5451,  6.1450,  2.0491,  4.5447,  5.9133,  2.1822,  8.6531,\n",
      "        17.1378,  7.9556,  8.6936,  9.8439,  2.4007,  6.7738,  8.0855,  5.4970,\n",
      "         5.5469,  4.2903,  8.3186,  1.6390,  5.6167,  3.1612,  9.2759,  4.1225,\n",
      "        18.3381,  7.2588,  7.6243,  5.6361,  3.4462,  6.2807,  7.5607,  7.0937,\n",
      "         5.8072,  6.0109, 10.2851,  5.7952, 10.4229,  3.7319,  7.6731,  7.1393,\n",
      "         8.7790,  7.9531,  6.0598,  6.4705, 12.4581,  9.7264,  5.5483, 10.1612,\n",
      "         2.2610,  9.4438,  8.2637,  5.4642, 10.0188,  4.9718,  3.7406,  4.1713,\n",
      "         4.7620,  9.6388, 11.7555, 11.4725,  1.7701,  6.3337,  7.8261,  5.6942,\n",
      "         8.4971,  6.6262,  6.6575,  9.2032,  3.8775,  5.0157, 11.3584,  5.7553,\n",
      "         5.1024,  6.2264,  3.9634,  3.3068, 12.7558, 11.4554, 10.3884,  3.7351,\n",
      "         7.2557,  5.9862,  5.9886,  8.6619,  3.9831,  7.5273,  6.4603,  7.3291,\n",
      "         4.2002,  5.4493,  6.1694,  2.5629,  8.0973,  9.9017,  7.2719,  6.0594,\n",
      "         3.2589,  5.0578,  5.3298,  9.9984,  2.7125,  6.3676,  9.0736,  2.0162,\n",
      "         4.8490,  6.3953,  5.5108,  3.0305,  4.7301,  9.3121,  7.1085,  6.7197,\n",
      "         6.5387, 11.6534, 12.1343,  4.4939,  8.0686,  3.6779,  3.5046,  5.9724,\n",
      "        11.6130,  7.9417,  6.7284, 11.0486,  4.8812, 10.4604,  8.2601,  6.9048,\n",
      "         9.6184, 12.6680, 12.3970,  6.1796, 22.1648,  4.1761, 11.4228,  5.6492,\n",
      "        12.0803, 12.7729,  4.8197,  5.4599,  6.0454,  3.8107,  3.6646,  1.1483,\n",
      "         5.6238,  7.8478, 13.6629,  6.9306,  4.2668,  4.6757,  4.9345,  3.2784,\n",
      "         8.1968,  9.0207,  8.6678,  8.9292, 17.1985,  5.8994, 10.7514, 11.9058,\n",
      "         5.8870,  4.4794,  6.9308,  4.0639,  5.5905,  6.8055,  4.8042,  4.9497,\n",
      "         4.8797,  9.1203,  3.7889,  3.5904, 14.1679,  7.7961,  4.6663,  9.8000,\n",
      "         5.0123, 12.1606, 10.9089, 11.7903,  3.1141,  9.7896,  0.8291,  7.2621,\n",
      "        15.4644,  3.1414,  8.8904, 10.8131,  4.8294,  9.0866,  4.1169, 12.5508,\n",
      "         7.0581,  4.1649,  5.0230,  4.9966,  6.5292,  9.1921, 17.8941,  3.4591,\n",
      "        11.3654,  2.2092,  3.0092,  6.9658,  7.0662, 10.7316, 16.5840,  6.9945,\n",
      "         6.9656,  9.6281,  5.5388,  6.4138,  8.4081,  5.4632,  6.2998,  6.8475,\n",
      "        13.2980, 16.5194,  5.1717,  8.6878,  7.6980,  7.4741,  5.6348,  6.8044,\n",
      "         8.7295,  9.6365,  6.8808,  3.4695,  7.7918,  7.8475,  5.6577,  6.5785,\n",
      "         6.9752,  3.1831,  8.3350,  4.3601,  5.9045,  5.6018, 11.9553,  5.2502,\n",
      "         3.6898, 12.3809,  3.7476,  8.6510,  7.6743,  7.3233,  7.6000, 13.1677,\n",
      "         8.6119,  8.7970,  6.3738,  6.7552,  6.2934, 13.8761,  2.2694, 10.3082,\n",
      "         8.9336,  5.8506,  6.8249,  7.0092,  5.9237, 19.0232, 10.0560,  4.9680,\n",
      "         7.7764,  4.1655,  4.5679, 15.2342,  7.0176,  4.6861,  8.2506,  7.0416,\n",
      "         9.9371,  4.3159,  5.3043,  7.5407,  9.6679,  3.2986,  4.0735,  8.6044,\n",
      "        15.2645,  8.4305,  5.5513,  5.0624,  4.6492,  5.0602,  9.3237, 14.8144,\n",
      "         6.9765,  3.6839,  5.8897,  5.9681, 11.4796, 13.7507,  7.9083,  6.8580,\n",
      "         5.2019, 11.7716,  6.2158, 13.1776,  4.9035,  8.7120,  5.2984,  6.2880,\n",
      "         6.0096,  7.0718, 15.7991,  9.7145, 22.8483,  9.1344,  4.0585,  7.1340,\n",
      "         5.6053,  1.6204,  3.4050,  5.7024,  6.4051, 11.7407,  4.8413,  5.1545,\n",
      "         5.0941, 13.1842,  6.3491,  4.6478,  6.2809,  6.1144, 11.3917,  4.0898,\n",
      "         4.3986,  4.0382,  4.3771,  7.0106, 15.2143,  5.8598,  2.5828,  9.6786,\n",
      "         8.1848,  5.0416,  9.9489,  4.9113,  3.9700,  6.3099,  3.1972,  5.4450,\n",
      "         6.7264,  5.7142,  6.2796,  2.0628,  4.9550,  5.2084,  4.0253,  5.3663,\n",
      "        12.8487, 14.8227,  8.8043,  7.1624,  4.4623,  4.3772,  5.5799, 11.2821,\n",
      "        11.7673,  5.5811,  4.2830,  4.7249,  6.3455,  7.8224,  4.6868, 13.0099,\n",
      "        13.4479,  1.7938,  5.8738,  7.6858,  6.8165,  3.4285, 12.4727,  3.9122,\n",
      "         2.7901,  5.6002,  3.4654,  5.2381,  2.3839,  4.1587,  2.8452,  6.6459,\n",
      "         6.2072, 14.8919,  3.1976,  7.0431,  6.5412,  2.0470,  5.3844,  5.3689,\n",
      "         6.2764, 16.8986,  4.9391,  6.1019,  4.0043,  3.8170,  4.6612,  6.0630,\n",
      "         5.7629,  2.5660,  8.1864,  9.5259,  7.6439,  7.0351,  4.4581,  5.7632,\n",
      "         6.8080,  6.1236,  8.0262,  7.5188,  7.7709,  6.4347, 10.1482,  2.4233,\n",
      "         4.4507,  5.3955,  2.6279,  5.8411,  2.4775,  4.5138,  3.9018,  1.5195,\n",
      "         8.9934,  5.1441,  9.8465,  9.5385,  6.0958,  5.4777, 13.3033, 13.8435,\n",
      "         6.0091,  8.0735,  7.4565, 11.1494, 10.4150,  4.9799, 11.7641,  3.4170,\n",
      "        10.2144,  6.9349,  3.3469,  9.8496,  4.4418,  4.3099,  6.8550,  7.2952,\n",
      "         8.6043,  4.5746,  5.4918, 11.2337,  8.8572,  4.6615,  9.2472, 16.0716])\n",
      "tensor([6.9858e-14, 7.6513e-14, 1.1278e-11, 6.7438e-12, 5.7355e-12, 1.7422e-13,\n",
      "        4.6390e-13, 1.7332e-08, 1.8909e-06, 2.2439e-11, 3.8042e-13, 9.8099e-13,\n",
      "        1.9970e-13, 2.9080e-12, 1.7618e-14, 3.4567e-14, 7.5358e-11, 1.8160e-10,\n",
      "        2.3268e-12, 3.5123e-12, 6.2559e-12, 2.2589e-09, 4.9770e-10, 1.5367e-08,\n",
      "        1.1374e-11, 6.1082e-15, 4.2376e-15, 2.4011e-14, 5.5087e-15, 9.2057e-10,\n",
      "        1.2278e-14, 2.6569e-15, 6.3412e-15, 1.3375e-13, 5.8028e-14, 1.0295e-14,\n",
      "        3.3567e-13, 3.9077e-14, 5.3012e-15, 3.9613e-14, 3.3030e-15, 1.9792e-15,\n",
      "        2.7680e-15, 3.4352e-14, 6.8275e-15, 1.4024e-14, 1.1191e-14, 9.5940e-15,\n",
      "        1.0901e-13, 3.3825e-14, 3.6525e-14, 7.3494e-13, 9.4681e-15, 1.9744e-14,\n",
      "        1.0405e-13, 7.8806e-15, 2.4405e-14, 2.2344e-15, 3.5793e-15, 5.6411e-15,\n",
      "        4.7279e-15, 1.1257e-13, 1.7663e-13, 7.1928e-15, 5.9153e-15, 7.0654e-15,\n",
      "        4.8619e-14, 4.7121e-14, 1.0831e-14, 8.2920e-16, 1.2120e-15, 4.7884e-16,\n",
      "        2.3767e-15, 3.2135e-15, 3.5035e-15, 4.3079e-15, 4.6365e-14, 4.5537e-15,\n",
      "        4.2136e-14, 2.2200e-15, 1.5881e-13, 9.4193e-11, 8.4242e-12, 1.5308e-11,\n",
      "        2.0438e-09, 4.7463e-12, 5.0472e-11, 7.3180e-13, 7.0983e-13, 8.9209e-10,\n",
      "        2.6126e-14, 2.7092e-13, 3.0127e-14, 2.7532e-11, 6.6514e-12, 8.2144e-14,\n",
      "        1.0558e-11, 2.0663e-11, 8.7332e-14, 5.1785e-10, 1.4944e-12, 4.0248e-14,\n",
      "        5.1004e-13, 1.0822e-13, 1.7684e-04, 2.4452e-09, 4.6447e-10, 1.1251e-14,\n",
      "        3.1396e-12, 3.2859e-13, 4.9716e-13, 5.9170e-15, 7.9177e-13, 9.1399e-15,\n",
      "        6.0660e-13, 2.0663e-13, 2.7675e-15, 2.7007e-14, 2.8001e-15, 2.8913e-15,\n",
      "        1.6957e-15, 1.4065e-14, 9.6656e-16, 2.4242e-15, 4.6472e-15, 2.5175e-14,\n",
      "        1.8727e-15, 4.0315e-11, 5.8241e-13, 3.2654e-10, 6.9254e-12, 1.0075e-12,\n",
      "        7.0069e-11, 1.8240e-14, 3.0065e-11, 2.1068e-13, 1.4628e-14, 1.4033e-13,\n",
      "        1.6590e-12, 2.2475e-13, 7.2500e-12, 6.2935e-14, 5.3024e-15, 4.0101e-13,\n",
      "        4.5422e-10, 4.3248e-10, 1.1532e-08, 5.0572e-15, 1.6733e-13, 2.5899e-12,\n",
      "        9.4806e-14, 1.4793e-04, 3.8642e-04, 1.8800e-05, 3.4494e-05, 2.0287e-06,\n",
      "        1.7447e-06, 1.1716e-02, 8.2680e-06, 2.7644e-11, 3.3175e-07, 2.6291e-09,\n",
      "        3.9850e-08, 1.5217e-11, 1.6343e-10, 4.3166e-13, 5.4218e-09, 3.8189e-09,\n",
      "        2.5597e-11, 2.6577e-05, 4.1415e-07, 9.5921e-09, 2.0849e-08, 3.1927e-05,\n",
      "        2.2778e-05, 7.2557e-09, 3.7800e-08, 2.2227e-09, 1.1623e-10, 1.7843e-06,\n",
      "        4.4834e-07, 1.8557e-09, 9.1315e-09, 7.3655e-10, 1.6112e-08, 4.0021e-06,\n",
      "        6.5546e-06, 1.1413e-08, 1.2916e-05, 1.8021e-08, 1.0614e-05, 1.4264e-10,\n",
      "        2.2520e-05, 4.3554e-06, 2.3036e-07, 4.7443e-07, 2.7724e-07, 1.6679e-08,\n",
      "        1.2028e-08, 2.6282e-05, 2.7957e-06, 2.8364e-08, 2.1829e-07, 9.7301e-03,\n",
      "        4.1497e-06, 2.0279e-09, 7.7498e-12, 4.2185e-06, 6.1355e-07, 9.8681e-11,\n",
      "        2.0069e-10, 2.2021e-11, 6.7206e-06, 6.4583e-10, 2.5984e-10, 5.0625e-06,\n",
      "        5.4671e-06, 6.1720e-06, 1.3065e-06, 2.1623e-06, 1.3359e-07, 1.5196e-11,\n",
      "        7.8963e-04, 1.3556e-06, 1.4039e-07, 2.8366e-08, 1.2159e-07, 4.2394e-06,\n",
      "        8.7458e-08, 1.3838e-05, 1.2246e-03, 6.0276e-03, 4.7528e-03, 2.1035e-09,\n",
      "        3.3181e-11, 2.5057e-06, 4.6111e-10, 6.0352e-11, 1.7794e-07, 7.1840e-06,\n",
      "        1.5080e-06, 5.8860e-08, 4.7858e-08, 8.6785e-10, 6.3856e-08, 8.2853e-07,\n",
      "        5.9947e-09, 7.4491e-06, 9.1059e-03, 5.8352e-03, 4.8465e-03, 1.2447e-06,\n",
      "        1.7675e-09, 3.4554e-07, 4.2946e-08, 6.1220e-09, 2.6211e-07, 8.2228e-03,\n",
      "        9.0527e-01, 4.1657e-03, 1.6059e-05, 4.2704e-04, 1.6838e-09, 5.4337e-04,\n",
      "        1.4156e-03, 4.0002e-07, 5.9864e-07, 1.1134e-06, 5.0059e-10, 1.2214e-05,\n",
      "        7.4379e-03, 4.4247e-08, 7.7272e-09, 3.2014e-06, 2.0696e-08, 6.8470e-09,\n",
      "        7.7930e-10, 1.9205e-07, 3.2683e-08, 1.6147e-02, 6.4060e-08, 1.4803e-06,\n",
      "        9.0784e-07, 8.0278e-05, 5.6130e-04, 3.5778e-06, 2.0214e-09, 1.2555e-05,\n",
      "        7.2657e-12, 7.9066e-09, 2.4285e-12, 2.1983e-10, 2.9435e-08, 2.2104e-10,\n",
      "        5.8234e-11, 7.5982e-11, 7.7441e-08, 1.2356e-10, 5.6776e-11, 1.1581e-10,\n",
      "        6.8691e-16, 2.5960e-14, 1.7140e-15, 9.1796e-16, 2.8024e-15, 3.1361e-15,\n",
      "        8.4379e-15, 9.1810e-16, 1.1762e-14, 1.1152e-12, 7.5159e-14, 2.4693e-15,\n",
      "        1.4234e-15, 8.6626e-15, 1.1575e-15, 3.6361e-15, 3.7994e-15, 1.1827e-15,\n",
      "        4.1752e-15, 1.1666e-15, 1.0952e-15, 5.5134e-14, 1.2419e-13, 6.0197e-15,\n",
      "        6.1814e-12, 1.1713e-14, 1.0301e-14, 1.2514e-11, 7.4777e-13, 9.9168e-13,\n",
      "        8.9395e-08, 8.6685e-07, 5.7306e-04, 3.1708e-06, 7.4681e-11, 5.5311e-09,\n",
      "        8.3426e-10, 1.6379e-11, 1.1260e-05, 3.5729e-11, 1.9145e-11, 1.6840e-07,\n",
      "        9.0737e-11, 2.3211e-12, 2.7005e-13, 3.8791e-09, 1.0686e-11, 1.3416e-13,\n",
      "        2.6133e-07, 3.3790e-10, 1.9397e-11, 2.3395e-13, 6.8630e-13, 2.5923e-10,\n",
      "        3.4961e-11, 4.5848e-06, 9.9412e-06, 1.8880e-09, 1.6626e-06, 3.7621e-06,\n",
      "        4.7563e-12, 5.9850e-09, 4.8189e-09, 1.5439e-12, 1.0739e-10, 2.4691e-12,\n",
      "        2.9533e-12, 6.4726e-13, 4.1521e-08, 6.9004e-12, 1.0336e-08, 8.2012e-09,\n",
      "        4.9427e-10, 1.5822e-08, 3.8951e-08, 2.1854e-09, 1.3390e-10, 4.6605e-08,\n",
      "        9.8207e-09, 9.0099e-12, 1.6661e-07, 3.4373e-10, 8.8960e-10, 1.0038e-07,\n",
      "        4.4942e-08, 6.1505e-14, 2.1430e-14, 1.6507e-09, 3.3513e-07, 1.2737e-12,\n",
      "        5.1900e-15, 1.6072e-13, 1.7001e-15, 6.3231e-14, 2.3224e-13, 4.9349e-14,\n",
      "        7.7324e-15, 1.6886e-12, 5.2806e-14, 1.2527e-13, 1.5705e-13, 7.6409e-14,\n",
      "        1.2601e-13, 5.9996e-15, 1.3781e-13, 6.4295e-14, 1.3536e-14, 9.3394e-14,\n",
      "        1.8596e-13, 2.9212e-14, 2.2206e-12, 1.2641e-13, 7.5284e-11, 1.8090e-13,\n",
      "        1.4336e-14, 3.3152e-13, 1.4748e-13, 1.3095e-13, 8.5257e-14, 8.4434e-12,\n",
      "        1.4765e-12, 6.4015e-14, 2.1151e-13, 1.6137e-14, 6.4298e-13, 1.3705e-13,\n",
      "        7.3542e-15, 8.6391e-14, 3.0485e-09, 1.9640e-12, 5.5849e-15, 1.2558e-10,\n",
      "        1.0117e-12, 3.0795e-14, 1.1535e-09, 1.6601e-10, 1.2778e-13, 3.1625e-13,\n",
      "        4.7337e-15, 8.5035e-12, 5.8051e-14, 6.0811e-14, 6.2748e-13, 2.8905e-11,\n",
      "        1.9266e-13, 1.4079e-13, 1.5756e-14, 2.2646e-12, 5.9782e-12, 2.6336e-14,\n",
      "        4.4430e-12, 3.2323e-15, 3.7841e-10, 5.4792e-12, 2.0767e-11, 4.0829e-13,\n",
      "        1.5208e-13, 3.1125e-09, 7.3131e-14, 1.6567e-13, 1.3411e-11, 6.6809e-13,\n",
      "        1.2809e-12, 1.1878e-08, 5.2712e-15, 8.2591e-14, 2.3790e-14, 4.9215e-14,\n",
      "        1.0345e-13, 1.2392e-12, 2.4166e-14, 5.2736e-12, 2.7542e-13, 1.4358e-14,\n",
      "        2.2637e-12, 8.4433e-12, 3.6526e-12, 2.4201e-15, 8.8900e-09, 5.1889e-13,\n",
      "        7.3085e-13, 1.3190e-14, 2.6471e-14, 1.8674e-12, 8.7531e-15, 6.1867e-14,\n",
      "        8.0792e-14, 2.7422e-12, 1.3630e-14, 5.9876e-12, 1.4991e-12, 6.3275e-13,\n",
      "        6.9242e-14, 8.1783e-16, 4.2833e-14, 4.2749e-14, 5.6498e-10, 1.0461e-12,\n",
      "        3.8865e-14, 8.4642e-14, 1.9757e-14, 1.9005e-10, 6.9755e-12, 6.7400e-14,\n",
      "        1.7355e-13, 6.8293e-14, 1.4641e-14, 2.3707e-15, 9.7776e-10, 9.3237e-15,\n",
      "        8.6298e-16, 1.5201e-13, 1.1492e-15, 2.9488e-13, 8.0648e-14, 5.9488e-11,\n",
      "        1.7832e-11, 3.5915e-15, 7.2221e-14, 3.3999e-11, 1.1592e-11, 1.6477e-13,\n",
      "        6.0810e-12, 8.0233e-12, 4.1473e-13, 1.2563e-12, 1.0325e-13, 2.7106e-13,\n",
      "        1.1144e-14, 2.7958e-11, 4.4849e-14, 5.3881e-14, 1.3357e-13, 2.2727e-14,\n",
      "        4.4203e-09, 8.6463e-15, 9.6888e-16, 1.3269e-08, 2.4909e-14, 1.8200e-07,\n",
      "        6.5554e-16, 7.3504e-14, 3.3832e-14, 9.6216e-13, 9.9732e-14, 9.0020e-15,\n",
      "        1.2118e-13, 2.0167e-15, 2.4458e-14, 9.6120e-14, 2.3036e-15, 1.4883e-12,\n",
      "        7.2038e-09, 7.4094e-13, 1.5499e-12, 4.8963e-12, 2.8662e-15, 2.2727e-13,\n",
      "        8.4375e-13, 6.3391e-14, 6.6637e-14, 1.8964e-14, 1.0652e-12, 1.3382e-15,\n",
      "        7.1449e-14, 6.1318e-15, 2.7744e-12, 1.6035e-14, 2.3924e-08, 3.6913e-13,\n",
      "        5.3198e-13, 7.2853e-14, 8.1538e-15, 1.3880e-13, 4.9919e-13, 3.1294e-13,\n",
      "        8.6441e-14, 1.0597e-13, 7.6113e-12, 8.5416e-14, 8.7362e-12, 1.0850e-14,\n",
      "        5.5861e-13, 3.2753e-13, 1.6880e-12, 7.3908e-13, 1.1129e-13, 1.6780e-13,\n",
      "        6.6863e-11, 4.3535e-12, 6.6725e-14, 6.7242e-12, 2.4926e-15, 3.2816e-12,\n",
      "        1.0083e-12, 6.1344e-14, 5.8322e-12, 3.7493e-14, 1.0946e-14, 1.6838e-14,\n",
      "        3.0396e-14, 3.9884e-12, 3.3117e-11, 2.4955e-11, 1.5256e-15, 1.4634e-13,\n",
      "        6.5090e-13, 7.7206e-14, 1.2733e-12, 1.9607e-13, 2.0231e-13, 2.5800e-12,\n",
      "        1.2551e-14, 3.9172e-14, 2.2263e-11, 8.2069e-14, 4.2720e-14, 1.3146e-13,\n",
      "        1.3676e-14, 7.0932e-15, 9.0051e-11, 2.4532e-11, 8.4399e-12, 1.0886e-14,\n",
      "        3.6798e-13, 1.0339e-13, 1.0364e-13, 1.5015e-12, 1.3949e-14, 4.8279e-13,\n",
      "        1.6611e-13, 3.9598e-13, 1.7332e-14, 6.0436e-14, 1.2418e-13, 3.3711e-15,\n",
      "        8.5376e-13, 5.1877e-12, 3.7398e-13, 1.1124e-13, 6.7613e-15, 4.0857e-14,\n",
      "        5.3632e-14, 5.7140e-12, 3.9149e-15, 1.5139e-13, 2.2664e-12, 1.9514e-15,\n",
      "        3.3159e-14, 1.5566e-13, 6.4273e-14, 5.3806e-15, 2.9440e-14, 2.8767e-12,\n",
      "        3.1759e-13, 2.1530e-13, 1.7965e-13, 2.9904e-11, 4.8368e-11, 2.3247e-14,\n",
      "        8.2957e-13, 1.0280e-14, 8.6443e-15, 1.0197e-13, 2.8719e-11, 7.3072e-13,\n",
      "        2.1717e-13, 1.6333e-11, 3.4245e-14, 9.0696e-12, 1.0047e-12, 2.5906e-13,\n",
      "        3.9075e-12, 8.2482e-11, 6.2901e-11, 1.2544e-13, 1.0984e-06, 1.6918e-14,\n",
      "        2.3745e-11, 7.3813e-14, 4.5826e-11, 9.1598e-11, 3.2200e-14, 6.1084e-14,\n",
      "        1.0969e-13, 1.1740e-14, 1.0144e-14, 8.1919e-16, 7.1957e-14, 6.6524e-13,\n",
      "        2.2307e-10, 2.6584e-13, 1.8525e-14, 2.7883e-14, 3.6120e-14, 6.8943e-15,\n",
      "        9.4307e-13, 2.1496e-12, 1.5104e-12, 1.9615e-12, 7.6542e-09, 9.4796e-14,\n",
      "        1.2133e-11, 3.8488e-11, 9.3621e-14, 2.2914e-14, 2.6589e-13, 1.5123e-14,\n",
      "        6.9601e-14, 2.3459e-13, 3.1706e-14, 3.6673e-14, 3.4192e-14, 2.3747e-12,\n",
      "        1.1486e-14, 9.4189e-15, 3.6962e-10, 6.3169e-13, 2.7621e-14, 4.6860e-12,\n",
      "        3.9042e-14, 4.9656e-11, 1.4203e-11, 3.4291e-11, 5.8499e-15, 4.6374e-12,\n",
      "        5.9538e-16, 3.7035e-13, 1.3515e-09, 6.0114e-15, 1.8870e-12, 1.2905e-11,\n",
      "        3.2514e-14, 2.2960e-12, 1.5946e-14, 7.3356e-11, 3.0198e-13, 1.6730e-14,\n",
      "        3.9459e-14, 3.8433e-14, 1.7795e-13, 2.5515e-12, 1.5347e-08, 8.2602e-15,\n",
      "        2.2420e-11, 2.3668e-15, 5.2673e-15, 2.7537e-13, 3.0445e-13, 1.1895e-11,\n",
      "        4.1406e-09, 2.8338e-13, 2.7531e-13, 3.9458e-12, 6.6098e-14, 1.5855e-13,\n",
      "        1.1650e-12, 6.1286e-14, 1.4147e-13, 2.4463e-13, 1.5486e-10, 3.8815e-09,\n",
      "        4.5786e-14, 1.5409e-12, 5.7269e-13, 4.5777e-13, 7.2752e-14, 2.3431e-13,\n",
      "        1.6064e-12, 3.9789e-12, 2.5293e-13, 8.3462e-15, 6.2899e-13, 6.6502e-13,\n",
      "        7.4438e-14, 1.8694e-13, 2.7797e-13, 6.2676e-15, 1.0828e-12, 2.0337e-14,\n",
      "        9.5275e-14, 7.0393e-14, 4.0441e-11, 4.9525e-14, 1.0403e-14, 6.1898e-11,\n",
      "        1.1022e-14, 1.4852e-12, 5.5923e-13, 3.9370e-13, 5.1923e-13, 1.3594e-10,\n",
      "        1.4283e-12, 1.7187e-12, 1.5234e-13, 2.2307e-13, 1.4058e-13, 2.7608e-10,\n",
      "        2.5136e-15, 7.7895e-12, 1.9701e-12, 9.0279e-14, 2.3918e-13, 2.8759e-13,\n",
      "        9.7123e-14, 4.7465e-08, 6.0532e-12, 3.7347e-14, 6.1938e-13, 1.6741e-14,\n",
      "        2.5033e-14, 1.0736e-09, 2.9002e-13, 2.8174e-14, 9.9520e-13, 2.9705e-13,\n",
      "        5.3745e-12, 1.9456e-14, 5.2280e-14, 4.8931e-13, 4.1058e-12, 7.0354e-15,\n",
      "        1.5268e-14, 1.4176e-12, 1.1066e-09, 1.1913e-12, 6.6927e-14, 4.1046e-14,\n",
      "        2.7154e-14, 4.0956e-14, 2.9103e-12, 7.0554e-10, 2.7832e-13, 1.0342e-14,\n",
      "        9.3882e-14, 1.0154e-13, 2.5133e-11, 2.4353e-10, 7.0667e-13, 2.4722e-13,\n",
      "        4.7192e-14, 3.3653e-11, 1.3008e-13, 1.3729e-10, 3.5015e-14, 1.5786e-12,\n",
      "        5.1971e-14, 1.3982e-13, 1.0584e-13, 3.0616e-13, 1.8887e-09, 4.3021e-12,\n",
      "        2.1757e-06, 2.4083e-12, 1.5041e-14, 3.2581e-13, 7.0643e-14, 1.3135e-15,\n",
      "        7.8250e-15, 7.7847e-14, 1.5718e-13, 3.2630e-11, 3.2904e-14, 4.5007e-14,\n",
      "        4.2367e-14, 1.3821e-10, 1.4862e-13, 2.7115e-14, 1.3882e-13, 1.1753e-13,\n",
      "        2.3017e-11, 1.5520e-14, 2.1135e-14, 1.4739e-14, 2.0686e-14, 2.8799e-13,\n",
      "        1.0524e-09, 9.1116e-14, 3.4389e-15, 4.1504e-12, 9.3181e-13, 4.0203e-14,\n",
      "        5.4382e-12, 3.5291e-14, 1.3768e-14, 1.4291e-13, 6.3566e-15, 6.0181e-14,\n",
      "        2.1675e-13, 7.8770e-14, 1.3865e-13, 2.0443e-15, 3.6867e-14, 4.7497e-14,\n",
      "        1.4551e-14, 5.5623e-14, 9.8815e-11, 7.1140e-10, 1.7313e-12, 3.3520e-13,\n",
      "        2.2524e-14, 2.0687e-14, 6.8866e-14, 2.0628e-11, 3.3511e-11, 6.8953e-14,\n",
      "        1.8828e-14, 2.9288e-14, 1.4809e-13, 6.4852e-13, 2.8193e-14, 1.1610e-10,\n",
      "        1.7990e-10, 1.5622e-15, 9.2396e-14, 5.6572e-13, 2.3718e-13, 8.0108e-15,\n",
      "        6.7849e-11, 1.2995e-14, 4.2309e-15, 7.0279e-14, 8.3119e-15, 4.8931e-14,\n",
      "        2.8184e-15, 1.6626e-14, 4.4707e-15, 1.9997e-13, 1.2896e-13, 7.6238e-10,\n",
      "        6.3592e-15, 2.9749e-13, 1.8010e-13, 2.0123e-15, 5.6639e-14, 5.5768e-14,\n",
      "        1.3820e-13, 5.6711e-09, 3.6285e-14, 1.1607e-13, 1.4248e-14, 1.1814e-14,\n",
      "        2.7482e-14, 1.1165e-13, 8.2695e-14, 3.3813e-15, 9.3327e-13, 3.5624e-12,\n",
      "        5.4250e-13, 2.9514e-13, 2.2429e-14, 8.2721e-14, 2.3517e-13, 1.1862e-13,\n",
      "        7.9513e-13, 4.7870e-13, 6.1597e-13, 1.6190e-13, 6.6374e-12, 2.9317e-15,\n",
      "        2.2266e-14, 5.7273e-14, 3.5973e-15, 8.9428e-14, 3.0951e-15, 2.3716e-14,\n",
      "        1.2860e-14, 1.1875e-15, 2.0916e-12, 4.4542e-14, 4.9088e-12, 3.6076e-12,\n",
      "        1.1536e-13, 6.2177e-14, 1.5569e-10, 2.6721e-10, 1.0578e-13, 8.3361e-13,\n",
      "        4.4978e-13, 1.8064e-11, 8.6668e-12, 3.7796e-14, 3.3404e-11, 7.9190e-15,\n",
      "        7.0920e-12, 2.6699e-13, 7.3833e-15, 4.9239e-12, 2.2068e-14, 1.9340e-14,\n",
      "        2.4648e-13, 3.8281e-13, 1.4175e-12, 2.5201e-14, 6.3059e-14, 1.9653e-11,\n",
      "        1.8253e-12, 2.7488e-14, 2.6960e-12, 2.4804e-09])\n"
     ]
    }
   ],
   "source": [
    "# sample execution (requires torchvision)\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "input_image = Image.open(filename)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "# move the input and model to GPU for speed if available\n",
    "if torch.cuda.is_available():\n",
    "    input_batch = input_batch.to('cuda')\n",
    "    model.to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)\n",
    "# Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\n",
    "print(output[0])\n",
    "# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
    "print(torch.nn.functional.softmax(output[0], dim=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Description\n",
    "\n",
    "Model `squeezenet1_0` is from the [SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size](https://arxiv.org/pdf/1602.07360.pdf) paper\n",
    "\n",
    "Model `squeezenet1_1` is from the [official squeezenet repo](https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1).\n",
    "It has 2.4x less computation and slightly fewer parameters than `squeezenet1_0`, without sacrificing accuracy.\n",
    "\n",
    "Their 1-crop error rates on imagenet dataset with pretrained models are listed below.\n",
    "\n",
    "| Model structure | Top-1 error | Top-5 error |\n",
    "| --------------- | ----------- | ----------- |\n",
    "|  squeezenet1_0  | 41.90       | 19.58       |\n",
    "|  squeezenet1_1  | 41.81       | 19.38       |\n",
    "\n",
    "### References\n",
    "\n",
    " - [Squeezenet: Alexnet-level accuracy with 50x fewer parameters and <0.5MB model size](https://arxiv.org/pdf/1602.07360.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"output/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-67cf5f5a0b52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdummy_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output/model.onnx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "dummy_input = Variable(torch.randn(1, 1, 28, 28)) \n",
    "torch.onnx.export(torch, dummy_input, \"output/model.onnx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'onnxpipeline'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-4c6f93019f3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./python_sdk'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0monnxpipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Initiate ONNX pipeline with your model path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'onnxpipeline'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./python_sdk')\n",
    "import onnxpipeline\n",
    "\n",
    "# Initiate ONNX pipeline with your model path\n",
    "pipeline = onnxpipeline.Pipeline(os.path.join('output', 'model.pt'))\n",
    "\n",
    "# Check the configs\n",
    "pipeline.config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the same process as Alexnet code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 28, 28)\n",
    "model = torchvision.models.squeezenet1_1(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%input.1 : Float(1, 3, 28, 28),\n",
      "      %features.0.weight : Float(64, 3, 3, 3),\n",
      "      %features.0.bias : Float(64),\n",
      "      %features.3.squeeze.weight : Float(16, 64, 1, 1),\n",
      "      %features.3.squeeze.bias : Float(16),\n",
      "      %features.3.expand1x1.weight : Float(64, 16, 1, 1),\n",
      "      %features.3.expand1x1.bias : Float(64),\n",
      "      %features.3.expand3x3.weight : Float(64, 16, 3, 3),\n",
      "      %features.3.expand3x3.bias : Float(64),\n",
      "      %features.4.squeeze.weight : Float(16, 128, 1, 1),\n",
      "      %features.4.squeeze.bias : Float(16),\n",
      "      %features.4.expand1x1.weight : Float(64, 16, 1, 1),\n",
      "      %features.4.expand1x1.bias : Float(64),\n",
      "      %features.4.expand3x3.weight : Float(64, 16, 3, 3),\n",
      "      %features.4.expand3x3.bias : Float(64),\n",
      "      %features.6.squeeze.weight : Float(32, 128, 1, 1),\n",
      "      %features.6.squeeze.bias : Float(32),\n",
      "      %features.6.expand1x1.weight : Float(128, 32, 1, 1),\n",
      "      %features.6.expand1x1.bias : Float(128),\n",
      "      %features.6.expand3x3.weight : Float(128, 32, 3, 3),\n",
      "      %features.6.expand3x3.bias : Float(128),\n",
      "      %features.7.squeeze.weight : Float(32, 256, 1, 1),\n",
      "      %features.7.squeeze.bias : Float(32),\n",
      "      %features.7.expand1x1.weight : Float(128, 32, 1, 1),\n",
      "      %features.7.expand1x1.bias : Float(128),\n",
      "      %features.7.expand3x3.weight : Float(128, 32, 3, 3),\n",
      "      %features.7.expand3x3.bias : Float(128),\n",
      "      %features.9.squeeze.weight : Float(48, 256, 1, 1),\n",
      "      %features.9.squeeze.bias : Float(48),\n",
      "      %features.9.expand1x1.weight : Float(192, 48, 1, 1),\n",
      "      %features.9.expand1x1.bias : Float(192),\n",
      "      %features.9.expand3x3.weight : Float(192, 48, 3, 3),\n",
      "      %features.9.expand3x3.bias : Float(192),\n",
      "      %features.10.squeeze.weight : Float(48, 384, 1, 1),\n",
      "      %features.10.squeeze.bias : Float(48),\n",
      "      %features.10.expand1x1.weight : Float(192, 48, 1, 1),\n",
      "      %features.10.expand1x1.bias : Float(192),\n",
      "      %features.10.expand3x3.weight : Float(192, 48, 3, 3),\n",
      "      %features.10.expand3x3.bias : Float(192),\n",
      "      %features.11.squeeze.weight : Float(64, 384, 1, 1),\n",
      "      %features.11.squeeze.bias : Float(64),\n",
      "      %features.11.expand1x1.weight : Float(256, 64, 1, 1),\n",
      "      %features.11.expand1x1.bias : Float(256),\n",
      "      %features.11.expand3x3.weight : Float(256, 64, 3, 3),\n",
      "      %features.11.expand3x3.bias : Float(256),\n",
      "      %features.12.squeeze.weight : Float(64, 512, 1, 1),\n",
      "      %features.12.squeeze.bias : Float(64),\n",
      "      %features.12.expand1x1.weight : Float(256, 64, 1, 1),\n",
      "      %features.12.expand1x1.bias : Float(256),\n",
      "      %features.12.expand3x3.weight : Float(256, 64, 3, 3),\n",
      "      %features.12.expand3x3.bias : Float(256),\n",
      "      %classifier.1.weight : Float(1000, 512, 1, 1),\n",
      "      %classifier.1.bias : Float(1000)):\n",
      "  %53 : Float(1, 64, 13, 13) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%input.1, %features.0.weight, %features.0.bias) # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:350:0\n",
      "  %54 : Float(1, 64, 13, 13) = onnx::Relu(%53) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1061:0\n",
      "  %55 : Float(1, 64, 6, 6) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 1, 1], strides=[2, 2]](%54) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:539:0\n",
      "  %56 : Float(1, 16, 6, 6) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%55, %features.3.squeeze.weight, %features.3.squeeze.bias) # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:350:0\n",
      "  %57 : Float(1, 16, 6, 6) = onnx::Relu(%56) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1061:0\n",
      "  %58 : Float(1, 64, 6, 6) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%57, %features.3.expand1x1.weight, %features.3.expand1x1.bias) # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:350:0\n",
      "  %59 : Float(1, 64, 6, 6) = onnx::Relu(%58) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1061:0\n",
      "  %60 : Float(1, 64, 6, 6) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%57, %features.3.expand3x3.weight, %features.3.expand3x3.bias) # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:350:0\n",
      "  %61 : Float(1, 64, 6, 6) = onnx::Relu(%60) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1061:0\n",
      "  %62 : Float(1, 128, 6, 6) = onnx::Concat[axis=1](%59, %61) # /usr/local/lib/python3.6/dist-packages/torchvision/models/squeezenet.py:34:0\n",
      "  %63 : Float(1, 16, 6, 6) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%62, %features.4.squeeze.weight, %features.4.squeeze.bias) # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:350:0\n",
      "  %64 : Float(1, 16, 6, 6) = onnx::Relu(%63) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1061:0\n",
      "  %65 : Float(1, 64, 6, 6) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%64, %features.4.expand1x1.weight, %features.4.expand1x1.bias) # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:350:0\n",
      "  %66 : Float(1, 64, 6, 6) = onnx::Relu(%65) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1061:0\n",
      "  %67 : Float(1, 64, 6, 6) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%64, %features.4.expand3x3.weight, %features.4.expand3x3.bias) # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:350:0\n",
      "  %68 : Float(1, 64, 6, 6) = onnx::Relu(%67) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1061:0\n",
      "  %69 : Float(1, 128, 6, 6) = onnx::Concat[axis=1](%66, %68) # /usr/local/lib/python3.6/dist-packages/torchvision/models/squeezenet.py:34:0\n",
      "  %70 : Float(1, 128, 3, 3) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 2, 2], strides=[2, 2]](%69) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:539:0\n",
      "  %71 : Float(1, 32, 3, 3) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%70, %features.6.squeeze.weight, %features.6.squeeze.bias) # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:350:0\n",
      "  %72 : Float(1, 32, 3, 3) = onnx::Relu(%71) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1061:0\n",
      "  %73 : Float(1, 128, 3, 3) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%72, %features.6.expand1x1.weight, %features.6.expand1x1.bias) # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:350:0\n",
      "  %74 : Float(1, 128, 3, 3) = onnx::Relu(%73) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1061:0\n",
      "  %75 : Float(1, 128, 3, 3) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%72, %features.6.expand3x3.weight, %features.6.expand3x3.bias) # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:350:0\n",
      "  %76 : Float(1, 128, 3, 3) = onnx::Relu(%75) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1061:0\n",
      "  %77 : Float(1, 256, 3, 3) = onnx::Concat[axis=1](%74, %76) # /usr/local/lib/python3.6/dist-packages/torchvision/models/squeezenet.py:34:0\n",
      "  %78 : Float(1, 32, 3, 3) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%77, %features.7.squeeze.weight, %features.7.squeeze.bias) # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:350:0\n",
      "  %79 : Float(1, 32, 3, 3) = onnx::Relu(%78) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1061:0\n",
      "  %80 : Float(1, 128, 3, 3) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%79, %features.7.expand1x1.weight, %features.7.expand1x1.bias) # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:350:0\n",
      "  %81 : Float(1, 128, 3, 3) = onnx::Relu(%80) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1061:0\n",
      "  %82 : Float(1, 128, 3, 3) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%79, %features.7.expand3x3.weight, %features.7.expand3x3.bias) # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:350:0\n",
      "  %83 : Float(1, 128, 3, 3) = onnx::Relu(%82) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1061:0\n",
      "  %84 : Float(1, 256, 3, 3) = onnx::Concat[axis=1](%81, %83) # /usr/local/lib/python3.6/dist-packages/torchvision/models/squeezenet.py:34:0\n",
      "  %85 : Float(1, 256, 1, 1) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 1, 1], strides=[2, 2]](%84) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:539:0\n",
      "  %86 : Float(1, 48, 1, 1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%85, %features.9.squeeze.weight, %features.9.squeeze.bias) # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:350:0\n",
      "  %87 : Float(1, 48, 1, 1) = onnx::Relu(%86) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1061:0\n",
      "  %88 : Float(1, 192, 1, 1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%87, %features.9.expand1x1.weight, %features.9.expand1x1.bias) # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:350:0\n",
      "  %89 : Float(1, 192, 1, 1) = onnx::Relu(%88) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1061:0\n",
      "  %90 : Float(1, 192, 1, 1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%87, %features.9.expand3x3.weight, %features.9.expand3x3.bias) # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:350:0\n",
      "  %91 : Float(1, 192, 1, 1) = onnx::Relu(%90) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1061:0\n",
      "  %92 : Float(1, 384, 1, 1) = onnx::Concat[axis=1](%89, %91) # /usr/local/lib/python3.6/dist-packages/torchvision/models/squeezenet.py:34:0\n",
      "  %93 : Float(1, 48, 1, 1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%92, %features.10.squeeze.weight, %features.10.squeeze.bias) # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:350:0\n",
      "  %94 : Float(1, 48, 1, 1) = onnx::Relu(%93) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1061:0\n",
      "  %95 : Float(1, 192, 1, 1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%94, %features.10.expand1x1.weight, %features.10.expand1x1.bias) # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:350:0\n",
      "  %96 : Float(1, 192, 1, 1) = onnx::Relu(%95) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1061:0\n",
      "  %97 : Float(1, 192, 1, 1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%94, %features.10.expand3x3.weight, %features.10.expand3x3.bias) # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:350:0\n",
      "  %98 : Float(1, 192, 1, 1) = onnx::Relu(%97) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1061:0\n",
      "  %99 : Float(1, 384, 1, 1) = onnx::Concat[axis=1](%96, %98) # /usr/local/lib/python3.6/dist-packages/torchvision/models/squeezenet.py:34:0\n",
      "  %100 : Float(1, 64, 1, 1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%99, %features.11.squeeze.weight, %features.11.squeeze.bias) # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:350:0\n",
      "  %101 : Float(1, 64, 1, 1) = onnx::Relu(%100) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1061:0\n",
      "  %102 : Float(1, 256, 1, 1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%101, %features.11.expand1x1.weight, %features.11.expand1x1.bias) # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:350:0\n",
      "  %103 : Float(1, 256, 1, 1) = onnx::Relu(%102) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1061:0\n",
      "  %104 : Float(1, 256, 1, 1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%101, %features.11.expand3x3.weight, %features.11.expand3x3.bias) # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:350:0\n",
      "  %105 : Float(1, 256, 1, 1) = onnx::Relu(%104) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1061:0\n",
      "  %106 : Float(1, 512, 1, 1) = onnx::Concat[axis=1](%103, %105) # /usr/local/lib/python3.6/dist-packages/torchvision/models/squeezenet.py:34:0\n",
      "  %107 : Float(1, 64, 1, 1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%106, %features.12.squeeze.weight, %features.12.squeeze.bias) # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:350:0\n",
      "  %108 : Float(1, 64, 1, 1) = onnx::Relu(%107) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1061:0\n",
      "  %109 : Float(1, 256, 1, 1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%108, %features.12.expand1x1.weight, %features.12.expand1x1.bias) # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:350:0\n",
      "  %110 : Float(1, 256, 1, 1) = onnx::Relu(%109) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1061:0\n",
      "  %111 : Float(1, 256, 1, 1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%108, %features.12.expand3x3.weight, %features.12.expand3x3.bias) # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:350:0\n",
      "  %112 : Float(1, 256, 1, 1) = onnx::Relu(%111) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1061:0\n",
      "  %113 : Float(1, 512, 1, 1) = onnx::Concat[axis=1](%110, %112) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n",
      "  %114 : Float(1, 1000, 1, 1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%113, %classifier.1.weight, %classifier.1.bias) # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:350:0\n",
      "  %115 : Float(1, 1000, 1, 1) = onnx::Relu(%114) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1061:0\n",
      "  %116 : Float(1, 1000, 1, 1) = onnx::GlobalAveragePool(%115) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:889:0\n",
      "  %117 : Float(1, 1000) = onnx::Flatten[axis=1](%116) # /usr/local/lib/python3.6/dist-packages/torchvision/models/squeezenet.py:102:0\n",
      "  return (%117)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(model, dummy_input, \"output/squeezenet.onnx\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "# Load the ONNX model\n",
    "model = onnx.load(\"output/squeezenet.onnx\")\n",
    "\n",
    "# Check that the IR is well formed\n",
    "#onnx.checker.check_model(model)\n",
    "\n",
    "# Print a human readable representation of the graph\n",
    "#onnx.helper.printable_graph(model.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: output/squeezenet-tf.pb/assets\n"
     ]
    }
   ],
   "source": [
    "from onnx_tf.backend import prepare\n",
    "tf_rep = prepare(model)\n",
    "tf_rep.export_graph(\"output/squeezenet-tf.pb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
